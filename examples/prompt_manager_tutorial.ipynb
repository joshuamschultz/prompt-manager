{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt Manager: Comprehensive Tutorial\n",
        "\n",
        "## ðŸŽ¯ What You'll Learn\n",
        "\n",
        "- Load and render prompts with dynamic variable substitution\n",
        "- Manage prompt versions and track changes over time\n",
        "- Use advanced template features (conditionals, loops, filters)\n",
        "- Validate inputs and outputs with JSON schemas\n",
        "- Integrate with popular LLM providers (Anthropic, OpenAI, LangChain, LiteLLM)\n",
        "- Implement custom storage backends and metadata tracking\n",
        "- Apply best practices for production prompt management\n",
        "\n",
        "## ðŸ§  Conceptual Foundation\n",
        "\n",
        "**The Problem**: In AI applications, prompts are scattered across code, making them hard to version, test, and maintain. Changes require code deployments, and non-technical team members can't iterate on prompts.\n",
        "\n",
        "**The Solution**: Prompt Manager separates prompts from code, storing them as versioned YAML files with:\n",
        "- **Template Engine**: Dynamic variable substitution with Jinja2\n",
        "- **Version Control**: Semantic versioning for prompt evolution\n",
        "- **Schema Validation**: Input/output validation for reliability\n",
        "- **Provider Integration**: Seamless integration with LLM APIs\n",
        "\n",
        "**Real-World Analogy**: Think of Prompt Manager like a CMS (Content Management System) for your AI prompts. Just as WordPress separates content from code, Prompt Manager separates prompts from your application logic.\n",
        "\n",
        "## ðŸ”§ What We're Building\n",
        "\n",
        "We'll build progressively complex examples:\n",
        "1. **Basic**: Load a greeting prompt and substitute variables\n",
        "2. **Intermediate**: Manage versions and apply conditional logic\n",
        "3. **Advanced**: Integrate with LLM providers and validate schemas\n",
        "4. **Production**: Implement custom storage and metadata tracking\n",
        "\n",
        "## ðŸ“‹ Prerequisites\n",
        "\n",
        "- Python 3.8+\n",
        "- Basic understanding of Python and JSON\n",
        "- Familiarity with LLMs (helpful but not required)\n",
        "- API keys for LLM providers (for integration examples)\n",
        "\n",
        "## ðŸ—‚ï¸ Tutorial Structure\n",
        "\n",
        "1. **Setup & Basic Usage**: Load and render your first prompt\n",
        "2. **Template Engine**: Variables, conditionals, loops, and filters\n",
        "3. **Version Management**: Track prompt evolution over time\n",
        "4. **Schema Validation**: Ensure input/output correctness\n",
        "5. **LLM Integrations**: Connect to Anthropic, OpenAI, LangChain, LiteLLM\n",
        "6. **Advanced Features**: Custom storage, metadata, discovery\n",
        "7. **Best Practices**: Production-ready patterns\n",
        "\n",
        "## âš¡ Quick Start (For Experienced Developers)\n",
        "\n",
        "Prompt Manager loads versioned prompt templates from YAML files, renders them with Jinja2, validates with JSON schemas, and integrates with major LLM providers. This tutorial shows you how to leverage all these features for production AI applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 1: Setup & Basic Usage\n",
        "\n",
        "## Understanding the Basics\n",
        "\n",
        "Prompt Manager uses a simple file structure:\n",
        "- **Prompts** stored as YAML files in a directory\n",
        "- **Versions** tracked automatically with semantic versioning\n",
        "- **Templates** use Jinja2 syntax for dynamic content\n",
        "\n",
        "Let's start by importing the library and loading our first prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the core PromptManager class\n",
        "# This is your main interface for loading and managing prompts\n",
        "from prompt_manager import PromptManager\n",
        "\n",
        "# Initialize with the path to your prompts directory\n",
        "# The examples/prompts folder contains sample prompts for this tutorial\n",
        "pm = PromptManager(prompts_dir=\"prompts\")\n",
        "\n",
        "print(\"âœ“ Prompt Manager initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What Just Happened?\n",
        "\n",
        "The `PromptManager` initialized and scanned the `prompts/` directory for YAML files:\n",
        "1. **Discovery**: Found all `.yaml` files in the directory\n",
        "2. **Loading**: Parsed each file's metadata (name, version, template)\n",
        "3. **Registration**: Made prompts available via the manager\n",
        "\n",
        "The prompts directory contains several examples:\n",
        "- `greeting.yaml` - Simple greeting template\n",
        "- `code_review.yaml` - Code review with conditionals\n",
        "- `data_analysis.yaml` - Data analysis with loops\n",
        "- `text_summarization.yaml` - Summarization with schema validation\n",
        "\n",
        "Let's load and render our first prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the greeting prompt\n",
        "# Returns a Template object with metadata and rendering capabilities\n",
        "greeting_template = pm.get_template(\"greeting\")\n",
        "\n",
        "# Render the template with variables\n",
        "# Variables are substituted using Jinja2 syntax\n",
        "rendered = greeting_template.render({\n",
        "    \"name\": \"Alice\",\n",
        "    \"role\": \"Data Scientist\"\n",
        "})\n",
        "\n",
        "print(rendered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Output\n",
        "\n",
        "The `render()` method:\n",
        "1. **Loaded the template** from `greeting.yaml`\n",
        "2. **Substituted variables** `{{ name }}` and `{{ role }}` with provided values\n",
        "3. **Returned the final text** ready to send to an LLM\n",
        "\n",
        "**Key Concept**: The prompt template is stored separately from your code. You can update the greeting message in the YAML file without changing your Python code.\n",
        "\n",
        "### Real-World Context\n",
        "\n",
        "In production applications:\n",
        "- **Marketing teams** can update email prompts without developer help\n",
        "- **Product managers** can A/B test different prompt variations\n",
        "- **Developers** can version prompts alongside code in git\n",
        "\n",
        "Let's examine the YAML file structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the template metadata\n",
        "# This shows all information stored in the YAML file\n",
        "print(f\"Name: {greeting_template.name}\")\n",
        "print(f\"Version: {greeting_template.version}\")\n",
        "print(f\"Description: {greeting_template.description}\")\n",
        "print(f\"Template:\\n{greeting_template.template}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### YAML File Structure\n",
        "\n",
        "The `greeting.yaml` file looks like this:\n",
        "\n",
        "```yaml\n",
        "name: greeting\n",
        "version: 1.0.0\n",
        "description: A simple greeting template\n",
        "template: |\n",
        "  Hello {{ name }}! Welcome to our platform.\n",
        "  Your role is: {{ role }}\n",
        "```\n",
        "\n",
        "**Components**:\n",
        "- `name`: Unique identifier for the prompt\n",
        "- `version`: Semantic version (MAJOR.MINOR.PATCH)\n",
        "- `description`: Human-readable explanation\n",
        "- `template`: The actual prompt text with Jinja2 variables\n",
        "\n",
        "---\n",
        "\n",
        "# Section 2: Template Engine Features\n",
        "\n",
        "## Understanding Jinja2 Templates\n",
        "\n",
        "Prompt Manager uses Jinja2, a powerful template engine with:\n",
        "- **Variables**: `{{ variable_name }}`\n",
        "- **Conditionals**: `{% if condition %} ... {% endif %}`\n",
        "- **Loops**: `{% for item in list %} ... {% endfor %}`\n",
        "- **Filters**: `{{ text | upper }}`\n",
        "\n",
        "Let's explore each feature with practical examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables and Conditionals\n",
        "\n",
        "The `code_review.yaml` template uses conditionals to customize output based on input:\n",
        "- Shows different messages based on code quality\n",
        "- Includes optional sections when data is available\n",
        "- Adapts tone based on severity\n",
        "\n",
        "Let's see it in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load code review template\n",
        "code_review = pm.get_template(\"code_review\")\n",
        "\n",
        "# Render with high severity issues\n",
        "review = code_review.render({\n",
        "    \"code\": \"def process(): return data\",\n",
        "    \"language\": \"Python\",\n",
        "    \"focus_areas\": [\"security\", \"performance\"],\n",
        "    \"severity\": \"high\"\n",
        "})\n",
        "\n",
        "print(review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Conditionals Work\n",
        "\n",
        "The template contains logic like:\n",
        "\n",
        "```jinja2\n",
        "{% if severity == \"high\" %}\n",
        "âš ï¸ CRITICAL REVIEW REQUIRED\n",
        "{% else %}\n",
        "Standard code review\n",
        "{% endif %}\n",
        "```\n",
        "\n",
        "This allows **one template** to handle multiple scenarios without code changes.\n",
        "\n",
        "**Business Value**:\n",
        "- Reduce prompt duplication\n",
        "- Maintain consistency across variations\n",
        "- Easier to update logic in one place"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loops for Repeated Content\n",
        "\n",
        "The `data_analysis.yaml` template uses loops to process lists:\n",
        "- Iterate over datasets or columns\n",
        "- Generate bullet points or numbered lists\n",
        "- Handle variable-length inputs\n",
        "\n",
        "Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data analysis template\n",
        "data_analysis = pm.get_template(\"data_analysis\")\n",
        "\n",
        "# Render with multiple datasets\n",
        "analysis = data_analysis.render({\n",
        "    \"datasets\": [\n",
        "        {\"name\": \"sales_2024\", \"rows\": 10000},\n",
        "        {\"name\": \"customers\", \"rows\": 5000}\n",
        "    ],\n",
        "    \"analysis_type\": \"trend\"\n",
        "})\n",
        "\n",
        "print(analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loop Syntax Explained\n",
        "\n",
        "The template uses:\n",
        "\n",
        "```jinja2\n",
        "{% for dataset in datasets %}\n",
        "- {{ dataset.name }}: {{ dataset.rows }} rows\n",
        "{% endfor %}\n",
        "```\n",
        "\n",
        "**Key Features**:\n",
        "- Handles any list length (1 to 1000+ items)\n",
        "- Access nested properties with dot notation\n",
        "- Use `loop.index` for numbering\n",
        "- `loop.first` and `loop.last` for special cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filters for Text Transformation\n",
        "\n",
        "Jinja2 filters modify variables inline:\n",
        "- `{{ text | upper }}` - Convert to uppercase\n",
        "- `{{ text | lower }}` - Convert to lowercase\n",
        "- `{{ text | title }}` - Title case\n",
        "- `{{ list | join(\", \") }}` - Join list items\n",
        "- `{{ number | round(2) }}` - Round numbers\n",
        "\n",
        "Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple template with filters\n",
        "from prompt_manager.core.template import Template\n",
        "\n",
        "template = Template(\n",
        "    name=\"filters_demo\",\n",
        "    version=\"1.0.0\",\n",
        "    template=\"\"\"\n",
        "    Name: {{ name | title }}\n",
        "    Tags: {{ tags | join(\", \") | upper }}\n",
        "    Score: {{ score | round(2) }}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result = template.render({\n",
        "    \"name\": \"john doe\",\n",
        "    \"tags\": [\"python\", \"ai\", \"data\"],\n",
        "    \"score\": 3.14159\n",
        "})\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Common Filter Patterns\n",
        "\n",
        "**Text Formatting**:\n",
        "```jinja2\n",
        "{{ company_name | upper }}  # IBM, GOOGLE\n",
        "{{ email | lower }}  # user@example.com\n",
        "```\n",
        "\n",
        "**List Handling**:\n",
        "```jinja2\n",
        "{{ items | length }}  # Count items\n",
        "{{ items | first }}  # Get first item\n",
        "{{ items | last }}  # Get last item\n",
        "```\n",
        "\n",
        "**Default Values**:\n",
        "```jinja2\n",
        "{{ optional_field | default(\"N/A\") }}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# Section 3: Version Management\n",
        "\n",
        "## Understanding Versioning\n",
        "\n",
        "Prompt Manager uses **semantic versioning** (MAJOR.MINOR.PATCH):\n",
        "- **MAJOR**: Breaking changes to prompt structure\n",
        "- **MINOR**: New features or content additions\n",
        "- **PATCH**: Bug fixes or minor tweaks\n",
        "\n",
        "**Why Version Prompts?**\n",
        "- Track changes over time\n",
        "- A/B test different versions\n",
        "- Roll back to previous versions\n",
        "- Maintain multiple versions for different use cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrieving Specific Versions\n",
        "\n",
        "You can load any version of a prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the latest version (default)\n",
        "latest = pm.get_template(\"marketing_email\")\n",
        "print(f\"Latest version: {latest.version}\")\n",
        "\n",
        "# Get a specific version\n",
        "v1_0 = pm.get_template(\"marketing_email\", version=\"1.0.0\")\n",
        "print(f\"Specific version: {v1_0.version}\")\n",
        "\n",
        "# List all available versions\n",
        "versions = pm.list_versions(\"marketing_email\")\n",
        "print(f\"\\nAvailable versions: {versions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version Storage Structure\n",
        "\n",
        "Prompt Manager stores versions in a structured directory:\n",
        "\n",
        "```\n",
        "prompts/\n",
        "â”œâ”€â”€ marketing_email.yaml          # Latest version metadata\n",
        "â””â”€â”€ marketing_email/\n",
        "    â”œâ”€â”€ 1.0.0.yaml               # Version 1.0.0\n",
        "    â”œâ”€â”€ 1.0.1.yaml               # Version 1.0.1\n",
        "    â””â”€â”€ _versions/\n",
        "        â””â”€â”€ metadata.json        # Version history\n",
        "```\n",
        "\n",
        "**How It Works**:\n",
        "1. Main YAML file points to current version\n",
        "2. Version files stored in subdirectory\n",
        "3. Metadata tracks version history and changes\n",
        "4. All versions accessible programmatically"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing Versions\n",
        "\n",
        "Let's compare different versions of the same prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load two versions\n",
        "v1 = pm.get_template(\"marketing_email\", version=\"1.0.0\")\n",
        "v2 = pm.get_template(\"marketing_email\", version=\"1.0.1\")\n",
        "\n",
        "# Render with same data\n",
        "data = {\n",
        "    \"product\": \"AI Assistant\",\n",
        "    \"customer_name\": \"Sarah\",\n",
        "    \"discount\": \"20%\"\n",
        "}\n",
        "\n",
        "print(\"Version 1.0.0:\")\n",
        "print(v1.render(data))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "print(\"Version 1.0.1:\")\n",
        "print(v2.render(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version Management Best Practices\n",
        "\n",
        "**When to Bump Version**:\n",
        "- **MAJOR (2.0.0)**: Changed required variables, removed sections\n",
        "- **MINOR (1.1.0)**: Added new optional sections, improved clarity\n",
        "- **PATCH (1.0.1)**: Fixed typos, minor wording changes\n",
        "\n",
        "**Production Pattern**:\n",
        "```python\n",
        "# Pin to specific version for stability\n",
        "prod_template = pm.get_template(\"email\", version=\"1.2.3\")\n",
        "\n",
        "# Or use latest for development\n",
        "dev_template = pm.get_template(\"email\")  # Latest\n",
        "```\n",
        "\n",
        "**A/B Testing Pattern**:\n",
        "```python\n",
        "import random\n",
        "\n",
        "# Randomly select version for testing\n",
        "version = random.choice([\"1.0.0\", \"1.1.0\"])\n",
        "template = pm.get_template(\"email\", version=version)\n",
        "# Track which version was used for analytics\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# Section 4: Schema Validation\n",
        "\n",
        "## Understanding Schema Validation\n",
        "\n",
        "Schema validation ensures:\n",
        "- **Input Validation**: Required variables are provided\n",
        "- **Type Safety**: Variables have correct types (string, number, list)\n",
        "- **Output Validation**: LLM responses match expected structure\n",
        "\n",
        "**Why Validate?**\n",
        "- Catch errors early in development\n",
        "- Ensure consistent LLM responses\n",
        "- Document expected data shapes\n",
        "- Enable automatic testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input Schema Validation\n",
        "\n",
        "Define expected input structure in your YAML file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The text_summarization template has input schema validation\n",
        "summarization = pm.get_template(\"text_summarization\")\n",
        "\n",
        "# Valid input - passes validation\n",
        "valid_input = {\n",
        "    \"text\": \"This is a long article about AI...\",\n",
        "    \"max_length\": 100,\n",
        "    \"style\": \"bullet_points\"\n",
        "}\n",
        "\n",
        "result = summarization.render(valid_input)\n",
        "print(\"âœ“ Valid input accepted\")\n",
        "print(result[:200] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Invalid input - fails validation\n",
        "try:\n",
        "    invalid_input = {\n",
        "        \"text\": \"Short text\",\n",
        "        # Missing required 'max_length' and 'style'\n",
        "    }\n",
        "    summarization.render(invalid_input)\n",
        "except Exception as e:\n",
        "    print(f\"âœ— Validation error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input Schema Structure\n",
        "\n",
        "In the YAML file:\n",
        "\n",
        "```yaml\n",
        "input_schema:\n",
        "  type: object\n",
        "  required:\n",
        "    - text\n",
        "    - max_length\n",
        "    - style\n",
        "  properties:\n",
        "    text:\n",
        "      type: string\n",
        "      description: Text to summarize\n",
        "    max_length:\n",
        "      type: integer\n",
        "      minimum: 50\n",
        "      maximum: 500\n",
        "    style:\n",
        "      type: string\n",
        "      enum: [\"paragraph\", \"bullet_points\", \"executive\"]\n",
        "```\n",
        "\n",
        "**Schema Benefits**:\n",
        "- Automatic validation before rendering\n",
        "- Clear documentation of requirements\n",
        "- Type checking (string, integer, boolean, array)\n",
        "- Constraints (min/max, enum values, patterns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output Schema Validation\n",
        "\n",
        "Validate LLM responses to ensure they match expected structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load schema validator\n",
        "from prompt_manager.validation.validators import SchemaValidator\n",
        "\n",
        "# Define expected output structure\n",
        "output_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"required\": [\"summary\", \"key_points\", \"word_count\"],\n",
        "    \"properties\": {\n",
        "        \"summary\": {\"type\": \"string\"},\n",
        "        \"key_points\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\"type\": \"string\"},\n",
        "            \"minItems\": 3\n",
        "        },\n",
        "        \"word_count\": {\"type\": \"integer\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "validator = SchemaValidator(output_schema)\n",
        "\n",
        "# Simulate LLM response\n",
        "llm_response = {\n",
        "    \"summary\": \"AI is transforming industries...\",\n",
        "    \"key_points\": [\n",
        "        \"Increased automation\",\n",
        "        \"Better decision making\",\n",
        "        \"Cost reduction\"\n",
        "    ],\n",
        "    \"word_count\": 89\n",
        "}\n",
        "\n",
        "# Validate response\n",
        "is_valid = validator.validate(llm_response)\n",
        "print(f\"Response valid: {is_valid}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Real-World Schema Validation\n",
        "\n",
        "**Use Case**: Customer Support Chatbot\n",
        "\n",
        "```python\n",
        "# Ensure chatbot always returns structured responses\n",
        "output_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"required\": [\"response\", \"sentiment\", \"requires_escalation\"],\n",
        "    \"properties\": {\n",
        "        \"response\": {\"type\": \"string\"},\n",
        "        \"sentiment\": {\"enum\": [\"positive\", \"neutral\", \"negative\"]},\n",
        "        \"requires_escalation\": {\"type\": \"boolean\"},\n",
        "        \"suggested_actions\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\"type\": \"string\"}\n",
        "        }\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "**Benefits**:\n",
        "- Consistent response structure for UI\n",
        "- Reliable sentiment analysis\n",
        "- Automatic escalation detection\n",
        "- Easy to parse and route responses\n",
        "\n",
        "---\n",
        "\n",
        "# Section 5: LLM Provider Integrations\n",
        "\n",
        "## Understanding Integrations\n",
        "\n",
        "Prompt Manager integrates with major LLM providers:\n",
        "- **Anthropic** (Claude): Advanced reasoning and long context\n",
        "- **OpenAI** (GPT): General-purpose models\n",
        "- **LangChain**: Framework for building LLM applications\n",
        "- **LiteLLM**: Unified interface for 100+ LLM providers\n",
        "\n",
        "Each integration handles:\n",
        "- API authentication\n",
        "- Message formatting\n",
        "- Response parsing\n",
        "- Error handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Anthropic Integration\n",
        "\n",
        "Use Claude models with Prompt Manager:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Anthropic integration\n",
        "from prompt_manager.integrations.anthropic import AnthropicIntegration\n",
        "import os\n",
        "\n",
        "# Initialize integration\n",
        "# Requires ANTHROPIC_API_KEY environment variable\n",
        "anthropic = AnthropicIntegration(\n",
        "    api_key=os.getenv(\"ANTHROPIC_API_KEY\")\n",
        ")\n",
        "\n",
        "# Load and render prompt\n",
        "template = pm.get_template(\"text_summarization\")\n",
        "prompt = template.render({\n",
        "    \"text\": \"Artificial intelligence is transforming industries...\",\n",
        "    \"max_length\": 100,\n",
        "    \"style\": \"bullet_points\"\n",
        "})\n",
        "\n",
        "# Send to Claude\n",
        "# response = anthropic.generate(\n",
        "#     prompt=prompt,\n",
        "#     model=\"claude-3-5-sonnet-20241022\",\n",
        "#     max_tokens=1024\n",
        "# )\n",
        "\n",
        "# print(response)\n",
        "\n",
        "print(\"Note: Uncomment code above and set ANTHROPIC_API_KEY to run\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Anthropic Integration Works\n",
        "\n",
        "Behind the scenes:\n",
        "1. **Authentication**: Uses API key from environment\n",
        "2. **Message Formatting**: Converts prompt to Claude's message format\n",
        "3. **API Call**: Sends request to Anthropic API\n",
        "4. **Response Parsing**: Extracts text from response\n",
        "5. **Error Handling**: Catches and formats API errors\n",
        "\n",
        "**Model Selection**:\n",
        "- `claude-3-5-sonnet-20241022`: Best for most tasks\n",
        "- `claude-3-5-haiku-20241022`: Fast and economical\n",
        "- `claude-3-opus-20240229`: Most capable, highest cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI Integration\n",
        "\n",
        "Use GPT models with the same pattern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import OpenAI integration\n",
        "from prompt_manager.integrations.openai import OpenAIIntegration\n",
        "\n",
        "# Initialize integration\n",
        "# Requires OPENAI_API_KEY environment variable\n",
        "openai = OpenAIIntegration(\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "# Load and render prompt\n",
        "template = pm.get_template(\"code_review\")\n",
        "prompt = template.render({\n",
        "    \"code\": \"def add(a, b): return a + b\",\n",
        "    \"language\": \"Python\",\n",
        "    \"focus_areas\": [\"best_practices\"],\n",
        "    \"severity\": \"low\"\n",
        "})\n",
        "\n",
        "# Send to GPT\n",
        "# response = openai.generate(\n",
        "#     prompt=prompt,\n",
        "#     model=\"gpt-4\",\n",
        "#     temperature=0.7\n",
        "# )\n",
        "\n",
        "# print(response)\n",
        "\n",
        "print(\"Note: Uncomment code above and set OPENAI_API_KEY to run\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI Configuration Options\n",
        "\n",
        "**Common Parameters**:\n",
        "- `model`: \"gpt-4\", \"gpt-3.5-turbo\", \"gpt-4-turbo\"\n",
        "- `temperature`: 0.0 (deterministic) to 2.0 (creative)\n",
        "- `max_tokens`: Maximum response length\n",
        "- `top_p`: Alternative to temperature for randomness\n",
        "- `frequency_penalty`: Reduce repetition (-2.0 to 2.0)\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "response = openai.generate(\n",
        "    prompt=prompt,\n",
        "    model=\"gpt-4\",\n",
        "    temperature=0.3,  # Low for factual responses\n",
        "    max_tokens=500,\n",
        "    frequency_penalty=0.5  # Reduce repetition\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangChain Integration\n",
        "\n",
        "Combine Prompt Manager with LangChain's ecosystem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import LangChain integration\n",
        "from prompt_manager.integrations.langchain import LangChainIntegration\n",
        "\n",
        "# Initialize integration\n",
        "langchain = LangChainIntegration(prompt_manager=pm)\n",
        "\n",
        "# Convert Prompt Manager template to LangChain prompt\n",
        "# lc_prompt = langchain.to_langchain_prompt(\"greeting\")\n",
        "\n",
        "# Use with LangChain chains\n",
        "# from langchain_openai import ChatOpenAI\n",
        "# from langchain.chains import LLMChain\n",
        "\n",
        "# llm = ChatOpenAI(model=\"gpt-4\")\n",
        "# chain = LLMChain(llm=llm, prompt=lc_prompt)\n",
        "\n",
        "# response = chain.run(name=\"Alice\", role=\"Developer\")\n",
        "# print(response)\n",
        "\n",
        "print(\"Note: LangChain integration enables use with chains, agents, and more\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LangChain Use Cases\n",
        "\n",
        "**Sequential Chains**:\n",
        "```python\n",
        "# Chain multiple prompts together\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# Step 1: Summarize\n",
        "summarize = langchain.to_langchain_prompt(\"text_summarization\")\n",
        "\n",
        "# Step 2: Analyze sentiment\n",
        "analyze = langchain.to_langchain_prompt(\"sentiment_analysis\")\n",
        "\n",
        "# Chain them\n",
        "chain = SequentialChain(\n",
        "    chains=[summarize_chain, analyze_chain]\n",
        ")\n",
        "```\n",
        "\n",
        "**Agents with Tools**:\n",
        "```python\n",
        "# Use prompts in agent workflows\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"CodeReview\",\n",
        "        func=code_review_chain.run,\n",
        "        description=\"Reviews code for quality\"\n",
        "    )\n",
        "]\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LiteLLM Integration\n",
        "\n",
        "Access 100+ LLM providers with one interface:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import LiteLLM integration\n",
        "from prompt_manager.integrations.litellm import LiteLLMIntegration\n",
        "\n",
        "# Initialize integration\n",
        "litellm = LiteLLMIntegration()\n",
        "\n",
        "# Use any provider with unified interface\n",
        "template = pm.get_template(\"greeting\")\n",
        "prompt = template.render({\"name\": \"Bob\", \"role\": \"Engineer\"})\n",
        "\n",
        "# OpenAI\n",
        "# response = litellm.generate(prompt, model=\"gpt-4\")\n",
        "\n",
        "# Anthropic\n",
        "# response = litellm.generate(prompt, model=\"claude-3-5-sonnet-20241022\")\n",
        "\n",
        "# Cohere\n",
        "# response = litellm.generate(prompt, model=\"command-nightly\")\n",
        "\n",
        "# Replicate\n",
        "# response = litellm.generate(prompt, model=\"replicate/llama-2-70b\")\n",
        "\n",
        "print(\"Note: LiteLLM enables switching providers without code changes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LiteLLM Benefits\n",
        "\n",
        "**Provider Flexibility**:\n",
        "- Switch providers with configuration change\n",
        "- Test multiple models easily\n",
        "- Reduce vendor lock-in\n",
        "- Fallback to alternative providers\n",
        "\n",
        "**Supported Providers** (100+):\n",
        "- OpenAI, Anthropic, Cohere, Replicate\n",
        "- Azure OpenAI, AWS Bedrock, Google VertexAI\n",
        "- HuggingFace, Together AI, Anyscale\n",
        "- And many more...\n",
        "\n",
        "**Cost Optimization**:\n",
        "```python\n",
        "# Route based on cost/performance\n",
        "if complexity == \"high\":\n",
        "    model = \"gpt-4\"  # Best quality\n",
        "elif complexity == \"medium\":\n",
        "    model = \"gpt-3.5-turbo\"  # Balanced\n",
        "else:\n",
        "    model = \"claude-3-5-haiku-20241022\"  # Fast and cheap\n",
        "\n",
        "response = litellm.generate(prompt, model=model)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# Section 6: Advanced Features\n",
        "\n",
        "## Custom Storage Backends\n",
        "\n",
        "Prompt Manager supports multiple storage options:\n",
        "- **File Storage**: Default YAML file-based storage\n",
        "- **Memory Storage**: In-memory for testing\n",
        "- **Custom Storage**: Implement your own backend\n",
        "\n",
        "**Why Custom Storage?**\n",
        "- Store prompts in database for dynamic updates\n",
        "- Integrate with CMS or admin panel\n",
        "- Use cloud storage (S3, GCS, Azure)\n",
        "- Implement access control and auditing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Memory Storage for Testing\n",
        "\n",
        "Use in-memory storage for unit tests:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import memory storage\n",
        "from prompt_manager.storage.memory import MemoryStorage\n",
        "from prompt_manager import PromptManager\n",
        "\n",
        "# Create in-memory storage\n",
        "storage = MemoryStorage()\n",
        "\n",
        "# Add prompts programmatically\n",
        "storage.save_template(\n",
        "    name=\"test_prompt\",\n",
        "    version=\"1.0.0\",\n",
        "    template=\"Hello {{ name }}!\",\n",
        "    description=\"Test prompt\"\n",
        ")\n",
        "\n",
        "# Use with PromptManager\n",
        "pm_memory = PromptManager(storage=storage)\n",
        "template = pm_memory.get_template(\"test_prompt\")\n",
        "result = template.render({\"name\": \"Test\"})\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Storage Implementation\n",
        "\n",
        "Create a database-backed storage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Custom database storage\n",
        "from prompt_manager.storage.base import BaseStorage\n",
        "from typing import Optional, List\n",
        "\n",
        "class DatabaseStorage(BaseStorage):\n",
        "    \"\"\"Store prompts in PostgreSQL/MySQL/etc.\"\"\"\n",
        "    \n",
        "    def __init__(self, db_connection):\n",
        "        self.db = db_connection\n",
        "    \n",
        "    def load_template(self, name: str, version: Optional[str] = None):\n",
        "        \"\"\"Load prompt from database\"\"\"\n",
        "        query = \"SELECT * FROM prompts WHERE name = %s\"\n",
        "        params = [name]\n",
        "        \n",
        "        if version:\n",
        "            query += \" AND version = %s\"\n",
        "            params.append(version)\n",
        "        else:\n",
        "            query += \" ORDER BY version DESC LIMIT 1\"\n",
        "        \n",
        "        # result = self.db.execute(query, params)\n",
        "        # return self._to_template(result)\n",
        "        pass\n",
        "    \n",
        "    def save_template(self, name: str, version: str, template: str, **metadata):\n",
        "        \"\"\"Save prompt to database\"\"\"\n",
        "        query = \"\"\"\n",
        "            INSERT INTO prompts (name, version, template, metadata)\n",
        "            VALUES (%s, %s, %s, %s)\n",
        "        \"\"\"\n",
        "        # self.db.execute(query, [name, version, template, metadata])\n",
        "        pass\n",
        "    \n",
        "    def list_versions(self, name: str) -> List[str]:\n",
        "        \"\"\"List all versions of a prompt\"\"\"\n",
        "        query = \"SELECT version FROM prompts WHERE name = %s ORDER BY version\"\n",
        "        # results = self.db.execute(query, [name])\n",
        "        # return [r['version'] for r in results]\n",
        "        pass\n",
        "\n",
        "# Usage:\n",
        "# db_storage = DatabaseStorage(db_connection)\n",
        "# pm_db = PromptManager(storage=db_storage)\n",
        "\n",
        "print(\"âœ“ Custom storage interface defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Storage Implementation Benefits\n",
        "\n",
        "**Database Storage**:\n",
        "- Real-time updates without deployments\n",
        "- Query and search prompts\n",
        "- Access control and permissions\n",
        "- Audit logs for compliance\n",
        "\n",
        "**Cloud Storage (S3/GCS)**:\n",
        "- Centralized prompt repository\n",
        "- Version control with object versioning\n",
        "- Scalable and durable\n",
        "- Multi-region access\n",
        "\n",
        "**CMS Integration**:\n",
        "- Non-technical users can edit prompts\n",
        "- Visual editor for templates\n",
        "- Approval workflows\n",
        "- Preview before publishing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metadata and Prompt Discovery\n",
        "\n",
        "Organize and discover prompts with metadata:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all available prompts\n",
        "prompts = pm.list_prompts()\n",
        "print(\"Available prompts:\")\n",
        "for prompt_name in prompts:\n",
        "    template = pm.get_template(prompt_name)\n",
        "    print(f\"  - {prompt_name} (v{template.version}): {template.description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding Rich Metadata\n",
        "\n",
        "Enhance prompts with tags, categories, and custom metadata:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example YAML with rich metadata:\n",
        "example_yaml = \"\"\"\n",
        "name: customer_onboarding\n",
        "version: 1.0.0\n",
        "description: Personalized welcome message for new customers\n",
        "\n",
        "# Organizational metadata\n",
        "metadata:\n",
        "  category: customer_success\n",
        "  tags:\n",
        "    - onboarding\n",
        "    - email\n",
        "    - personalization\n",
        "  owner: customer_success_team\n",
        "  last_updated: 2024-01-15\n",
        "  estimated_tokens: 150\n",
        "  language: en\n",
        "  audience: b2b\n",
        "\n",
        "# Input schema\n",
        "input_schema:\n",
        "  type: object\n",
        "  required: [customer_name, plan_type]\n",
        "  properties:\n",
        "    customer_name:\n",
        "      type: string\n",
        "    plan_type:\n",
        "      type: string\n",
        "      enum: [starter, professional, enterprise]\n",
        "\n",
        "# Template\n",
        "template: |\n",
        "  Welcome {{ customer_name }}!\n",
        "  \n",
        "  {% if plan_type == \"enterprise\" %}\n",
        "  As an Enterprise customer, you have access to premium features...\n",
        "  {% else %}\n",
        "  Thank you for choosing our {{ plan_type }} plan...\n",
        "  {% endif %}\n",
        "\"\"\"\n",
        "\n",
        "print(\"Example YAML with rich metadata:\")\n",
        "print(example_yaml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filtering and Search\n",
        "\n",
        "Find prompts by metadata:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom filter function\n",
        "def find_prompts_by_tag(pm, tag):\n",
        "    \"\"\"Find all prompts with a specific tag\"\"\"\n",
        "    matching = []\n",
        "    \n",
        "    for prompt_name in pm.list_prompts():\n",
        "        template = pm.get_template(prompt_name)\n",
        "        metadata = getattr(template, 'metadata', {})\n",
        "        tags = metadata.get('tags', [])\n",
        "        \n",
        "        if tag in tags:\n",
        "            matching.append({\n",
        "                'name': prompt_name,\n",
        "                'version': template.version,\n",
        "                'description': template.description\n",
        "            })\n",
        "    \n",
        "    return matching\n",
        "\n",
        "# Example usage\n",
        "# email_prompts = find_prompts_by_tag(pm, \"email\")\n",
        "# print(f\"Found {len(email_prompts)} email prompts\")\n",
        "\n",
        "print(\"âœ“ Search and filter utilities defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metadata Best Practices\n",
        "\n",
        "**Standard Fields**:\n",
        "```yaml\n",
        "metadata:\n",
        "  # Organization\n",
        "  category: \"marketing\" | \"support\" | \"sales\" | \"product\"\n",
        "  tags: [\"email\", \"onboarding\", \"personalization\"]\n",
        "  owner: \"team_name\" or \"person@company.com\"\n",
        "  \n",
        "  # Technical\n",
        "  estimated_tokens: 150  # For cost estimation\n",
        "  model_compatibility: [\"gpt-4\", \"claude-3-5-sonnet-20241022\"]\n",
        "  language: \"en\" | \"es\" | \"fr\"\n",
        "  \n",
        "  # Lifecycle\n",
        "  created: \"2024-01-01\"\n",
        "  last_updated: \"2024-01-15\"\n",
        "  deprecated: false\n",
        "  replacement: \"new_prompt_name\"  # If deprecated\n",
        "```\n",
        "\n",
        "**Discovery Patterns**:\n",
        "- Category-based navigation\n",
        "- Tag-based search\n",
        "- Owner-based filtering\n",
        "- Token-based cost estimation\n",
        "\n",
        "---\n",
        "\n",
        "# Section 7: Best Practices\n",
        "\n",
        "## Production-Ready Patterns\n",
        "\n",
        "### Error Handling\n",
        "\n",
        "Always handle potential errors gracefully:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Production error handling pattern\n",
        "from prompt_manager.core.exceptions import TemplateNotFoundError, ValidationError\n",
        "\n",
        "def safe_render(pm, prompt_name, variables, version=None):\n",
        "    \"\"\"Safely render a prompt with error handling\"\"\"\n",
        "    try:\n",
        "        # Load template\n",
        "        template = pm.get_template(prompt_name, version=version)\n",
        "        \n",
        "        # Render with variables\n",
        "        result = template.render(variables)\n",
        "        \n",
        "        return {\"success\": True, \"result\": result}\n",
        "        \n",
        "    except TemplateNotFoundError:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": f\"Prompt '{prompt_name}' not found\"\n",
        "        }\n",
        "    \n",
        "    except ValidationError as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": f\"Validation failed: {str(e)}\"\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": f\"Unexpected error: {str(e)}\"\n",
        "        }\n",
        "\n",
        "# Usage\n",
        "result = safe_render(pm, \"greeting\", {\"name\": \"Alice\", \"role\": \"Developer\"})\n",
        "if result[\"success\"]:\n",
        "    print(result[\"result\"])\n",
        "else:\n",
        "    print(f\"Error: {result['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration Management\n",
        "\n",
        "Centralize configuration for different environments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration pattern\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class PromptConfig:\n",
        "    \"\"\"Configuration for prompt management\"\"\"\n",
        "    prompts_dir: str\n",
        "    default_version: str = \"latest\"\n",
        "    cache_enabled: bool = True\n",
        "    validation_enabled: bool = True\n",
        "    \n",
        "    # LLM provider configs\n",
        "    anthropic_api_key: str = None\n",
        "    openai_api_key: str = None\n",
        "    \n",
        "    # Performance\n",
        "    max_retries: int = 3\n",
        "    timeout: int = 30\n",
        "    \n",
        "    @classmethod\n",
        "    def from_env(cls):\n",
        "        \"\"\"Load configuration from environment\"\"\"\n",
        "        return cls(\n",
        "            prompts_dir=os.getenv(\"PROMPTS_DIR\", \"prompts\"),\n",
        "            anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
        "            openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "            cache_enabled=os.getenv(\"CACHE_ENABLED\", \"true\").lower() == \"true\",\n",
        "        )\n",
        "\n",
        "# Load configuration\n",
        "config = PromptConfig.from_env()\n",
        "pm_configured = PromptManager(prompts_dir=config.prompts_dir)\n",
        "\n",
        "print(f\"âœ“ Configured with prompts_dir: {config.prompts_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Caching Strategy\n",
        "\n",
        "Cache rendered prompts for performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple caching implementation\n",
        "from functools import lru_cache\n",
        "import hashlib\n",
        "import json\n",
        "\n",
        "class CachedPromptManager:\n",
        "    \"\"\"PromptManager with caching\"\"\"\n",
        "    \n",
        "    def __init__(self, prompts_dir):\n",
        "        self.pm = PromptManager(prompts_dir=prompts_dir)\n",
        "    \n",
        "    def _cache_key(self, prompt_name, variables, version=None):\n",
        "        \"\"\"Generate cache key from inputs\"\"\"\n",
        "        key_data = {\n",
        "            \"prompt\": prompt_name,\n",
        "            \"version\": version or \"latest\",\n",
        "            \"vars\": variables\n",
        "        }\n",
        "        key_str = json.dumps(key_data, sort_keys=True)\n",
        "        return hashlib.md5(key_str.encode()).hexdigest()\n",
        "    \n",
        "    @lru_cache(maxsize=1000)\n",
        "    def get_template_cached(self, prompt_name, version=None):\n",
        "        \"\"\"Cache template loading\"\"\"\n",
        "        return self.pm.get_template(prompt_name, version=version)\n",
        "    \n",
        "    def render(self, prompt_name, variables, version=None):\n",
        "        \"\"\"Render with caching\"\"\"\n",
        "        template = self.get_template_cached(prompt_name, version)\n",
        "        return template.render(variables)\n",
        "\n",
        "# Usage\n",
        "cached_pm = CachedPromptManager(\"prompts\")\n",
        "result = cached_pm.render(\"greeting\", {\"name\": \"Bob\", \"role\": \"Engineer\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitoring and Logging\n",
        "\n",
        "Track prompt usage and performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitoring wrapper\n",
        "import time\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MonitoredPromptManager:\n",
        "    \"\"\"PromptManager with monitoring\"\"\"\n",
        "    \n",
        "    def __init__(self, prompts_dir):\n",
        "        self.pm = PromptManager(prompts_dir=prompts_dir)\n",
        "        self.metrics = {\n",
        "            \"total_renders\": 0,\n",
        "            \"total_errors\": 0,\n",
        "            \"avg_render_time\": 0\n",
        "        }\n",
        "    \n",
        "    def render(self, prompt_name, variables, version=None):\n",
        "        \"\"\"Render with monitoring\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            template = self.pm.get_template(prompt_name, version=version)\n",
        "            result = template.render(variables)\n",
        "            \n",
        "            # Track success\n",
        "            duration = time.time() - start_time\n",
        "            self.metrics[\"total_renders\"] += 1\n",
        "            self._update_avg_time(duration)\n",
        "            \n",
        "            logger.info(f\"Rendered {prompt_name} in {duration:.3f}s\")\n",
        "            return result\n",
        "            \n",
        "        except Exception as e:\n",
        "            # Track error\n",
        "            self.metrics[\"total_errors\"] += 1\n",
        "            logger.error(f\"Error rendering {prompt_name}: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def _update_avg_time(self, duration):\n",
        "        \"\"\"Update rolling average\"\"\"\n",
        "        count = self.metrics[\"total_renders\"]\n",
        "        current_avg = self.metrics[\"avg_render_time\"]\n",
        "        self.metrics[\"avg_render_time\"] = (\n",
        "            (current_avg * (count - 1) + duration) / count\n",
        "        )\n",
        "    \n",
        "    def get_metrics(self):\n",
        "        \"\"\"Get current metrics\"\"\"\n",
        "        return self.metrics.copy()\n",
        "\n",
        "# Usage\n",
        "monitored_pm = MonitoredPromptManager(\"prompts\")\n",
        "result = monitored_pm.render(\"greeting\", {\"name\": \"Charlie\", \"role\": \"Manager\"})\n",
        "print(f\"\\nMetrics: {monitored_pm.get_metrics()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Strategy\n",
        "\n",
        "Comprehensive testing for prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing pattern\n",
        "import unittest\n",
        "\n",
        "class TestPrompts(unittest.TestCase):\n",
        "    \"\"\"Test prompt rendering and validation\"\"\"\n",
        "    \n",
        "    def setUp(self):\n",
        "        \"\"\"Initialize test PromptManager\"\"\"\n",
        "        self.pm = PromptManager(prompts_dir=\"prompts\")\n",
        "    \n",
        "    def test_greeting_render(self):\n",
        "        \"\"\"Test greeting prompt renders correctly\"\"\"\n",
        "        template = self.pm.get_template(\"greeting\")\n",
        "        result = template.render({\"name\": \"Test\", \"role\": \"Tester\"})\n",
        "        \n",
        "        self.assertIn(\"Test\", result)\n",
        "        self.assertIn(\"Tester\", result)\n",
        "    \n",
        "    def test_missing_variable(self):\n",
        "        \"\"\"Test error on missing required variable\"\"\"\n",
        "        template = self.pm.get_template(\"greeting\")\n",
        "        \n",
        "        with self.assertRaises(Exception):\n",
        "            template.render({\"name\": \"Test\"})  # Missing 'role'\n",
        "    \n",
        "    def test_version_loading(self):\n",
        "        \"\"\"Test loading specific version\"\"\"\n",
        "        v1 = self.pm.get_template(\"marketing_email\", version=\"1.0.0\")\n",
        "        self.assertEqual(v1.version, \"1.0.0\")\n",
        "    \n",
        "    def test_schema_validation(self):\n",
        "        \"\"\"Test input schema validation\"\"\"\n",
        "        template = self.pm.get_template(\"text_summarization\")\n",
        "        \n",
        "        # Valid input\n",
        "        valid = {\n",
        "            \"text\": \"Long text...\",\n",
        "            \"max_length\": 100,\n",
        "            \"style\": \"bullet_points\"\n",
        "        }\n",
        "        result = template.render(valid)\n",
        "        self.assertIsNotNone(result)\n",
        "\n",
        "# Run tests\n",
        "# suite = unittest.TestLoader().loadTestsFromTestCase(TestPrompts)\n",
        "# unittest.TextTestRunner(verbosity=2).run(suite)\n",
        "\n",
        "print(\"âœ“ Test suite defined (uncomment to run)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deployment Checklist\n",
        "\n",
        "**Before Production**:\n",
        "\n",
        "1. **Version Control**\n",
        "   - [ ] All prompts have semantic versions\n",
        "   - [ ] Version history documented\n",
        "   - [ ] Breaking changes noted in descriptions\n",
        "\n",
        "2. **Validation**\n",
        "   - [ ] Input schemas defined for all prompts\n",
        "   - [ ] Output schemas defined where applicable\n",
        "   - [ ] Validation enabled in production\n",
        "\n",
        "3. **Testing**\n",
        "   - [ ] Unit tests for all prompts\n",
        "   - [ ] Integration tests with LLM providers\n",
        "   - [ ] Edge cases covered\n",
        "   - [ ] Performance benchmarks established\n",
        "\n",
        "4. **Monitoring**\n",
        "   - [ ] Logging configured\n",
        "   - [ ] Metrics tracking implemented\n",
        "   - [ ] Error alerting set up\n",
        "   - [ ] Cost tracking enabled\n",
        "\n",
        "5. **Documentation**\n",
        "   - [ ] Prompt descriptions complete\n",
        "   - [ ] Metadata properly tagged\n",
        "   - [ ] Usage examples provided\n",
        "   - [ ] Team trained on system\n",
        "\n",
        "6. **Security**\n",
        "   - [ ] API keys in environment variables\n",
        "   - [ ] No sensitive data in prompts\n",
        "   - [ ] Access control implemented\n",
        "   - [ ] Audit logging enabled\n",
        "\n",
        "---\n",
        "\n",
        "# Summary and Key Takeaways\n",
        "\n",
        "## What You've Learned\n",
        "\n",
        "### Core Concepts\n",
        "1. **Separation of Concerns**: Prompts live in YAML files, not code\n",
        "2. **Version Management**: Track prompt evolution with semantic versioning\n",
        "3. **Template Engine**: Dynamic content with variables, conditionals, loops\n",
        "4. **Schema Validation**: Ensure correctness with JSON schemas\n",
        "5. **Provider Integration**: Seamless connection to LLM APIs\n",
        "\n",
        "### Key Benefits\n",
        "- **Maintainability**: Update prompts without code deployments\n",
        "- **Collaboration**: Non-technical users can edit prompts\n",
        "- **Reliability**: Validation prevents runtime errors\n",
        "- **Flexibility**: Switch providers without code changes\n",
        "- **Scalability**: Organized prompt library as you grow\n",
        "\n",
        "### Production Patterns\n",
        "- Error handling with graceful fallbacks\n",
        "- Configuration management for environments\n",
        "- Caching for performance\n",
        "- Monitoring and logging for observability\n",
        "- Comprehensive testing strategy\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "### Immediate Actions\n",
        "1. **Start Small**: Convert 1-2 prompts from code to YAML\n",
        "2. **Add Validation**: Define input schemas for safety\n",
        "3. **Test Integration**: Connect with your LLM provider\n",
        "4. **Monitor Usage**: Add basic logging and metrics\n",
        "\n",
        "### Growth Path\n",
        "1. **Expand Library**: Move all prompts to Prompt Manager\n",
        "2. **Team Onboarding**: Train team on prompt editing\n",
        "3. **Advanced Features**: Custom storage, rich metadata\n",
        "4. **Optimization**: Caching, A/B testing, cost tracking\n",
        "\n",
        "## Resources\n",
        "\n",
        "- **Documentation**: [GitHub Repository]\n",
        "- **Examples**: `/examples/prompts/` directory\n",
        "- **Community**: [Discord/Slack/Forum]\n",
        "- **Issues**: [GitHub Issues]\n",
        "\n",
        "## Common Questions\n",
        "\n",
        "**Q: Can I use Prompt Manager with existing code?**\n",
        "A: Yes! Migrate incrementallyâ€”no need to refactor everything at once.\n",
        "\n",
        "**Q: How do I handle sensitive prompts?**\n",
        "A: Use custom storage with access control, or encrypt YAML files.\n",
        "\n",
        "**Q: Can I version prompts with git?**\n",
        "A: Absolutely! YAML files work great with version control.\n",
        "\n",
        "**Q: What if a template fails to render?**\n",
        "A: Use error handling patterns from Section 7 for graceful degradation.\n",
        "\n",
        "**Q: How do I optimize costs?**\n",
        "A: Use metadata to track token estimates and route to cost-effective models.\n",
        "\n",
        "---\n",
        "\n",
        "## Final Example: Complete Workflow\n",
        "\n",
        "Here's a complete example bringing everything together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete production workflow\n",
        "from prompt_manager import PromptManager\n",
        "from prompt_manager.integrations.anthropic import AnthropicIntegration\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# Setup\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize\n",
        "pm = PromptManager(prompts_dir=\"prompts\")\n",
        "anthropic = AnthropicIntegration(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
        "\n",
        "def analyze_text(text, analysis_type=\"summary\"):\n",
        "    \"\"\"\n",
        "    Complete workflow: load, validate, render, send to LLM\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Load template with version pinning for stability\n",
        "        template = pm.get_template(\n",
        "            \"text_summarization\",\n",
        "            version=\"1.0.0\"  # Pin version for production\n",
        "        )\n",
        "        \n",
        "        # 2. Prepare input (will be validated by schema)\n",
        "        input_data = {\n",
        "            \"text\": text,\n",
        "            \"max_length\": 150,\n",
        "            \"style\": \"bullet_points\"\n",
        "        }\n",
        "        \n",
        "        # 3. Render prompt\n",
        "        prompt = template.render(input_data)\n",
        "        logger.info(f\"Rendered prompt for {analysis_type}\")\n",
        "        \n",
        "        # 4. Send to LLM (uncomment when API key is set)\n",
        "        # response = anthropic.generate(\n",
        "        #     prompt=prompt,\n",
        "        #     model=\"claude-3-5-sonnet-20241022\",\n",
        "        #     max_tokens=1024\n",
        "        # )\n",
        "        \n",
        "        # 5. Validate output (optional)\n",
        "        # validate_output(response)\n",
        "        \n",
        "        # 6. Return result\n",
        "        # return {\"success\": True, \"result\": response}\n",
        "        \n",
        "        return {\"success\": True, \"prompt\": prompt[:200] + \"...\"}\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Analysis failed: {e}\")\n",
        "        return {\"success\": False, \"error\": str(e)}\n",
        "\n",
        "# Example usage\n",
        "result = analyze_text(\n",
        "    \"Artificial intelligence is transforming industries by automating tasks, \"\n",
        "    \"improving decision-making, and enabling new capabilities that were \"\n",
        "    \"previously impossible. From healthcare to finance, AI is creating value.\"\n",
        ")\n",
        "\n",
        "if result[\"success\"]:\n",
        "    print(\"âœ“ Analysis complete!\")\n",
        "    print(f\"Prompt preview: {result['prompt']}\")\n",
        "else:\n",
        "    print(f\"âœ— Error: {result['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## You're Ready!\n",
        "\n",
        "You now have a complete understanding of Prompt Manager:\n",
        "- Loading and rendering prompts\n",
        "- Managing versions\n",
        "- Using template features\n",
        "- Validating inputs and outputs\n",
        "- Integrating with LLM providers\n",
        "- Implementing production patterns\n",
        "\n",
        "**Start building amazing AI applications with maintainable, version-controlled prompts!**\n",
        "\n",
        "---\n",
        "\n",
        "*Tutorial created with Prompt Manager v1.0.0*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
